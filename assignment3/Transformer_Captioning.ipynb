{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 29673,
          "status": "ok",
          "timestamp": 1764599260161,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "-XHTF-_Z1tS2",
        "outputId": "7430ad78-5db1-40fd-a1a9-cb36388dcf3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: torch.manual_seed seems to depend the underlying GPU which differs from\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "try:\n",
        "  from google.colab import drive # type: ignore\n",
        "  drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "  raise ImportError(\"WARNING: torch.manual_seed seems to depend the underlying GPU \"\n",
        "                  \"which differs from Google Colab. hense, \"\n",
        "                  \"YOU WILL GET ERRORS EVEN THE IMPLEMENTATION IS CORRECT.\")\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = 'cs231n/assignment3/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This downloads the COCO dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "# # !bash get_datasets.sh\n",
        "# !bash get_coco_dataset.sh\n",
        "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvYckCI7qyX",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Image Captioning with Transformers\n",
        "You have now implemented a vanilla RNN and for the task of image captioning. In this notebook you will implement key pieces of a transformer decoder to accomplish the same task.\n",
        "\n",
        "**NOTE:** This notebook will be primarily written in PyTorch rather than NumPy, unlike the RNN notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "executionInfo": {
          "elapsed": 35891,
          "status": "ok",
          "timestamp": 1764599296053,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "FXCykho37qya",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Setup cell.\n",
        "import time, os, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from cs231n.transformer_layers import *\n",
        "from cs231n.captioning_solver_transformer import CaptioningSolverTransformer\n",
        "from cs231n.classifiers.transformer import CaptioningTransformer\n",
        "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
        "from cs231n.image_utils import image_from_url\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "def rel_error(x:np.ndarray, y:np.ndarray):\n",
        "  \"\"\" returns relative error \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "# def rel_error(x, y):\n",
        "#     \"\"\" returns relative error, I changed this from 1e-8 to 1e-2; seems random seed changed a bit.? \"\"\"\n",
        "#     out:np.ndarray = ((np.abs(x - y) / (np.abs(x) + np.abs(y))))\n",
        "#     err = np.mean(out >= 1e-1) # type: ignore\n",
        "#     warn = np.mean(out >= 1e-2) # type: ignore\n",
        "#     return format(\"error: %.3f, warn: %.3f\" % (err, warn)) if warn < 0.99 else \"All wrong; may also be seed issues instead of implementation error.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fczxReWM7qyd"
      },
      "source": [
        "# COCO Dataset\n",
        "As in the previous notebooks, we will use the COCO dataset for captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 22443,
          "status": "ok",
          "timestamp": 1764599318493,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "lCKchFVJ7qye",
        "outputId": "9235efdb-661f-4653-fb16-85721bfa3e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base dir  /content/drive/MyDrive/cs231n/assignment3/cs231n/datasets/coco_captioning\n",
            "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
            "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
            "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
            "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
            "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
            "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
            "idx_to_word <class 'list'> 1004\n",
            "word_to_idx <class 'dict'> 1004\n",
            "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
            "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
          ]
        }
      ],
      "source": [
        "# Load COCO data from disk into a dictionary.\n",
        "data = load_coco_data(pca_features=True)\n",
        "\n",
        "# Print out all the keys and values from the data dictionary.\n",
        "for k, v in data.items():\n",
        "    if type(v) == np.ndarray:\n",
        "        print(k, type(v), v.shape, v.dtype)\n",
        "    else:\n",
        "        print(k, type(v), len(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go1XDgIn7qyg"
      },
      "source": [
        "# Transformer\n",
        "As you have seen, RNNs are incredibly powerful but often slow to train. Further, RNNs struggle to encode long-range dependencies (though LSTMs are one way of mitigating the issue). In 2017, Vaswani et al introduced the Transformer in their paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) to a) introduce parallelism and b) allow models to learn long-range dependencies. The paper not only led to famous models like BERT and GPT in the natural language processing community, but also an explosion of interest across fields, including vision. While here we introduce the model in the context of image captioning, the idea of attention itself is much more general.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPMDm4F9m0v"
      },
      "source": [
        "# Transformer: Multi-Headed Attention\n",
        "\n",
        "### Dot-Product Attention\n",
        "\n",
        "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\dots,v_n\\}, v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\dots,k_n\\}, k_i \\in \\mathbb{R}^d$, specified as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91g6XLTr7qyi"
      },
      "source": [
        "\\begin{align}\n",
        "c &= \\sum_{i=1}^{n} v_i \\alpha_i \\\\\n",
        "\\alpha_i &= \\frac{\\exp(k_i^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2wcdeS7qyj"
      },
      "source": [
        "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.\n",
        "\n",
        "### Self-Attention\n",
        "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V,K,Q \\in \\mathbb{R}^{d\\times d}$ to map our input $X$ as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2JDsjFU7qyl"
      },
      "source": [
        "\\begin{align}\n",
        "v_i = Vx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "k_i = Kx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "q_i = Qx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5GNjXoW7qyn"
      },
      "source": [
        "### Multi-Headed Scaled Dot-Product Attention\n",
        "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $h$ be number of heads, and $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRyHp98f7qyo"
      },
      "source": [
        "$$  \n",
        "Y_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)(XV_i)\n",
        "\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKHMc3g7qyo"
      },
      "source": [
        "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n",
        "\n",
        "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFFO4uu7qyp"
      },
      "source": [
        "$$  \n",
        "Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i)\n",
        "\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlyhrBW7qyp"
      },
      "source": [
        "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFR0osPf7qyq"
      },
      "source": [
        "$$  \n",
        "Y = [Y_1;\\dots;Y_h]A\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4G9fKfW7qyq"
      },
      "source": [
        "were $A \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n",
        "\n",
        "Implement multi-headed scaled dot-product attention in the `MultiHeadAttention` class in the file `cs231n/transformer_layers.py`. The code below will check your implementation. The relative error should be less than `e-3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 199,
          "status": "ok",
          "timestamp": 1764587235854,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "VeixCEKF7qyr",
        "outputId": "7c0d2f27-e6a7-4ba6-bb30-5e0f6a9553bf",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self_attn_output error:  0.0003775124598178026\n",
            "masked_self_attn_output error:  0.0001526367643724865\n",
            "attn_output error:  0.0003530104862933477\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "\n",
        "# Choose dimensions such that they are all unique for easier debugging:\n",
        "# Specifically, the following values correspond to N=1, H=2, T=3, E//H=4, and E=8.\n",
        "batch_size = 1\n",
        "sequence_length = 3\n",
        "embed_dim = 8\n",
        "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
        "\n",
        "# Self-attention.\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "self_attn_output = attn(query=data, key=data, value=data)\n",
        "\n",
        "# Masked self-attention.\n",
        "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
        "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
        "\n",
        "# Attention using two inputs.\n",
        "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "attn_output = attn(query=data, key=other_data, value=other_data)\n",
        "\n",
        "expected_self_attn_output = np.asarray([[\n",
        "[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,\n",
        "          -0.1444],\n",
        "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
        "          -0.2476],\n",
        "         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,\n",
        "          -0.3038]]])\n",
        "\n",
        "expected_masked_self_attn_output = np.asarray([[\n",
        "[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
        "          -0.3019],\n",
        "         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,\n",
        "          -0.1496],\n",
        "         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,\n",
        "          -0.2208]]])\n",
        "\n",
        "expected_attn_output = np.asarray([[\n",
        "[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,\n",
        "          -0.2744],\n",
        "         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,\n",
        "          -0.2767],\n",
        "         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,\n",
        "          -0.2912]]])\n",
        "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
        "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
        "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDBRnqL9m0w"
      },
      "source": [
        "# Positional Encoding\n",
        "\n",
        "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
        "\n",
        "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zawv8vzV7qyt"
      },
      "source": [
        "$$\n",
        "\\begin{cases}\n",
        "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
        "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0fhlupT7qyt"
      },
      "source": [
        "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$.\n",
        "\n",
        "Implement this layer in `PositionalEncoding` in `cs231n/transformer_layers.py`. Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `e-3` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 47,
          "status": "ok",
          "timestamp": 1764587245243,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "gi7px0XK7qyu",
        "outputId": "923d3f17-5c4a-427c-97b4-62f27ed214d2",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe_output error:  0.00010421011374914356\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 2\n",
        "embed_dim = 6\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "\n",
        "pos_encoder = PositionalEncoding(embed_dim)\n",
        "output = pos_encoder(data)\n",
        "\n",
        "expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n",
        "                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n",
        "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDoSUJ7y7qyv",
        "tags": [
          "pdf-inline"
        ]
      },
      "source": [
        "# Inline Question 1\n",
        "\n",
        "Several key design decisions were made in designing the scaled dot product attention we introduced above. Explain why the following choices were beneficial:\n",
        "1. Using multiple attention heads as opposed to one.\n",
        "2. Dividing by $\\sqrt{d/h}$ before applying the softmax function. Recall that $d$ is the feature dimension and $h$ is the number of heads.\n",
        "3. Adding a linear transformation to the output of the attention operation.\n",
        "\n",
        "Only one or two sentences per choice is necessary, but be sure to be specific in addressing what would have happened without each given implementation detail, why such a situation would be suboptimal, and how the proposed implementation improves the situation.\n",
        "\n",
        "**Your Answer:**\n",
        "\n",
        "1. Using multiple attention heads allows the model to attend to information from different representation subspaces at different positions, capturing diverse aspects of the input data.\n",
        "\n",
        "2. avoid gradient vanishing/exploding issues when the dot products grow too large in magnitude or small\n",
        "\n",
        "3. mix information from different heads, make the model learn complex relationships and interactions between different parts of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfRjeETn2TU"
      },
      "source": [
        "# Transformer Decoder Block\n",
        "\n",
        "Transformer decoder layer consists of three modules: (1) self attention to process input sequence of vectors, (2) cross attention to process based on available context (i.e. image features in our case), (3) feedforward module to process each vector of the sequence independently. Complete the implementation of `TransformerDecoderLayer` in `cs231n/transformer_layers.py` and test it below. The relative error should be less than 1e-6.\n",
        "\n",
        "The Transformer decoder layer has three main components: (1) a self-attention module that processes the input sequence of vectors, (2) a cross-attention module that incorporates additional context (e.g., image features in our case), and (3) a feedforward module that independently processes each vector in the sequence. Complete the implementation of `TransformerDecoderLayer` in `cs231n/transformer_layers.py` and test it below. The relative error should be less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 389,
          "status": "ok",
          "timestamp": 1764591799545,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "JHyiLXjwpU4q",
        "outputId": "2b854e09-d5b5-4e4e-b366-eef714667a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error:  4.044097834095886e-06\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "decoder_layer = TransformerDecoderLayer(D, 2, 4*D)\n",
        "tgt = torch.randn(N, T, D)\n",
        "memory = torch.randn(N, TM, D)\n",
        "tgt_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = decoder_layer(tgt, memory, tgt_mask)\n",
        "\n",
        "expected_output = np.asarray([\n",
        "    [[ 1.1464597, -0.32541496,  0.39171425, -0.39425734,  0.62471056,\n",
        "      -1.8665842, -0.12977494, -1.6609063, -0.5620399,  0.45006236,\n",
        "       1.6086785,  0.7173523],\n",
        "     [-0.6703264,  0.34731007, -0.01452054, -0.0500976,  0.9617562,\n",
        "      -0.91788256,  0.5138556, -1.5247818,  2.0940537, -1.0386938,\n",
        "       1.0333964, -0.7340692],\n",
        "     [-1.1966342,  0.78882384,  0.1765188,  0.04164891,  1.9480462,\n",
        "      -0.94358695,  0.83423877, -0.44660965,  1.1469632, -1.6658922,\n",
        "      -0.27915588, -0.4043607],\n",
        "     [-0.96863323,  0.10736976, -0.18560877, -0.86474127, -0.12873,\n",
        "       0.36593518,  0.9634492, -0.9432319,  1.4652547,  1.2200648,\n",
        "       0.9218512, -1.9529796]]\n",
        "])\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxBcIdRT7vvz"
      },
      "source": [
        "# Transformer for Image Captioning\n",
        "Now that you have implemented the previous layers, you can combine them to build a Transformer-based image captioning model. Open the file `cs231n/classifiers/transformer.py` and look at the `CaptioningTransformer` class.\n",
        "\n",
        "Implement the `forward` function of the class. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-5` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 55,
          "status": "ok",
          "timestamp": 1764594432224,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "W3Vxnysk72q6",
        "outputId": "16238649-093d-4543-cb14-bcac7e5dd326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scores error:  3.4445903163390594e-07\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, D, W = 4, 20, 30\n",
        "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
        "V = len(word_to_idx)\n",
        "T = 3\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "    word_to_idx,\n",
        "    input_dim=D,\n",
        "    wordvec_dim=W,\n",
        "    num_heads=2,\n",
        "    num_layers=2,\n",
        "    max_length=30\n",
        ")\n",
        "\n",
        "features = torch.randn(N, D)\n",
        "captions = torch.randint(0, V, (N, T))\n",
        "\n",
        "scores = transformer(features, captions)\n",
        "expected_scores = np.asarray([\n",
        "    [[ 0.48119992, -0.24859881, -0.7489549 ],\n",
        "     [ 0.20380056,  0.08959456, -0.89954275],\n",
        "     [ 0.21135767, -0.17083111, -0.62508506]],\n",
        "\n",
        "    [[ 0.49413955, -0.50489324, -0.79341394],\n",
        "     [ 0.87452495, -0.4392967 , -1.1513498 ],\n",
        "     [ 0.2547267 , -0.26321974, -0.93643296]],\n",
        "\n",
        "    [[ 0.70437765, -0.5729916 , -0.7946507 ],\n",
        "     [ 0.18345363, -0.31752932, -1.7304884 ],\n",
        "     [ 0.61473167, -0.82634443, -1.2179294 ]],\n",
        "\n",
        "    [[ 0.5163983 , -0.7899667 , -1.0383208 ],\n",
        "     [ 0.28063023, -0.3603301 , -1.5435203 ],\n",
        "     [ 0.7222998 , -0.71457165, -0.76669186]]\n",
        "])\n",
        "\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwUhSxEx7qyv"
      },
      "source": [
        "# Overfit Transformer Captioning Model on Small Data\n",
        "Run the following to overfit the Transformer-based captioning model on the same small dataset as we used for the RNN previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "executionInfo": {
          "elapsed": 37067,
          "status": "ok",
          "timestamp": 1764594495446,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "-HMqJq4T7qyv",
        "outputId": "ac486483-5368-4611-cf92-cb683167d60e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base dir  /content/drive/MyDrive/cs231n/assignment3/cs231n/datasets/coco_captioning\n",
            "(Iteration 1 / 200) loss: 5.055427\n",
            "(Iteration 11 / 200) loss: 2.819050\n",
            "(Iteration 21 / 200) loss: 2.036877\n",
            "(Iteration 31 / 200) loss: 1.659048\n",
            "(Iteration 41 / 200) loss: 1.313489\n",
            "(Iteration 51 / 200) loss: 1.195412\n",
            "(Iteration 61 / 200) loss: 0.864514\n",
            "(Iteration 71 / 200) loss: 0.806814\n",
            "(Iteration 81 / 200) loss: 0.566810\n",
            "(Iteration 91 / 200) loss: 0.435672\n",
            "(Iteration 101 / 200) loss: 0.380263\n",
            "(Iteration 111 / 200) loss: 0.157928\n",
            "(Iteration 121 / 200) loss: 0.111748\n",
            "(Iteration 131 / 200) loss: 0.102176\n",
            "(Iteration 141 / 200) loss: 0.077055\n",
            "(Iteration 151 / 200) loss: 0.072140\n",
            "(Iteration 161 / 200) loss: 0.046706\n",
            "(Iteration 171 / 200) loss: 0.039755\n",
            "(Iteration 181 / 200) loss: 0.028434\n",
            "(Iteration 191 / 200) loss: 0.027322\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "data = load_coco_data(max_train=50)\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "          word_to_idx=data['word_to_idx'],\n",
        "          input_dim=data['train_features'].shape[1],\n",
        "          wordvec_dim=256,\n",
        "          num_heads=2,\n",
        "          num_layers=2,\n",
        "          max_length=30\n",
        "        )\n",
        "\n",
        "\n",
        "transformer_solver = CaptioningSolverTransformer(transformer, data, idx_to_word=data['idx_to_word'],\n",
        "           num_epochs=100,\n",
        "           batch_size=25,\n",
        "           learning_rate=0.001,\n",
        "           verbose=True, print_every=10,\n",
        "         )\n",
        "\n",
        "transformer_solver.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(transformer_solver.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRNyolJ7qyw"
      },
      "source": [
        "Print final training loss. You should see a final loss of less than 0.05 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 122,
          "status": "ok",
          "timestamp": 1764594500478,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "JPU95Nv27qyx",
        "outputId": "3230062e-d1d1-4374-a0f6-21a318a2c8fc",
        "tags": [],
        "test": "transformer_final_training_loss"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final loss:  0.02068659\n"
          ]
        }
      ],
      "source": [
        "print('Final loss: ', transformer_solver.loss_history[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R-SUFxf7qyx"
      },
      "source": [
        "# Transformer Sampling at Test Time\n",
        "The sampling code has been written for you. You can simply run the following to compare with the previous results with the RNN. As before the training results should be much better than the validation set results, given how little data we trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "output_embedded_package_id": "1VCmTZQ5PYg0Fy72RnX6VIlUepTg1680c"
        },
        "executionInfo": {
          "elapsed": 3204,
          "status": "ok",
          "timestamp": 1764594611346,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "K4uQMkIC7qyy",
        "outputId": "34da0fe4-3851-4289-80c8-027b295eea30",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Output hidden; open in https://colab.research.google.com to view."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# If you get an error, the URL just no longer exists, so don't worry!\n",
        "# You can re-sample as many times as you want.\n",
        "for split in ['train', 'val']:\n",
        "    minibatch = sample_coco_minibatch(data, split=split, batch_size=2)\n",
        "    gt_captions, features, urls = minibatch\n",
        "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
        "\n",
        "    sample_captions = transformer.sample(features, max_length=30)\n",
        "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
        "\n",
        "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
        "        img = image_from_url(url)\n",
        "        # Skip missing URLs.\n",
        "        if img is None: continue\n",
        "        plt.imshow(img)\n",
        "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMSyzMWyQpK"
      },
      "source": [
        "# Vision Transformer (ViT)\n",
        "\n",
        "[Dosovitskiy et. al.](https://arxiv.org/abs/2010.11929) showed that applying a transformer model on a sequence of image patches (referred to as Vision Transformer) not only achieves impressive performance but also scales more effectively than convolutional neural networks when trained on large datasets. We will build a version of Vision Transformer using our existing implementation of transformer components and train it on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAntTB4F0yHC"
      },
      "source": [
        "Vision Transformer converts input image into a sequence of patches of fixed size and embed each patch into a latent vector. In `cs231n/transformer_layers.py`, complete the implementation of `PatchEmbedding` and test it below. You should see relative error less than 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1906,
          "status": "ok",
          "timestamp": 1764596135727,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "dRiRu3pN7qyz",
        "outputId": "37aa18eb-88ca-4fa6-9953-9d3ea26760a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error:  9.182286955268188e-08\n"
          ]
        }
      ],
      "source": [
        "from cs231n.transformer_layers import PatchEmbedding\n",
        "\n",
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N = 2\n",
        "HW = 16\n",
        "PS = 8\n",
        "D = 8\n",
        "\n",
        "patch_embedding = PatchEmbedding(\n",
        "    img_size=HW,\n",
        "    patch_size=PS,\n",
        "    embed_dim=D\n",
        ")\n",
        "\n",
        "x = torch.randn(N, 3, HW, HW)\n",
        "output = patch_embedding(x)\n",
        "\n",
        "\n",
        "expected_output = np.asarray([\n",
        "        [[-0.6312704 ,  0.02531429,  0.6112642 , -0.49089882,\n",
        "          0.01412961, -0.6959372 , -0.32862484, -0.45402682],\n",
        "        [ 0.18816411, -0.08142513, -0.9829535 , -0.23975623,\n",
        "         -0.23109074,  0.97950286, -0.40997326,  0.7457837 ],\n",
        "        [ 0.01810865,  0.15780598, -0.91804236,  0.36185235,\n",
        "          0.8379501 ,  1.0191797 , -0.29667392,  0.20322265],\n",
        "        [-0.18697818, -0.45137224, -0.40339014, -1.4381214 ,\n",
        "         -0.43450755,  0.7651071 , -0.83683825, -0.16360264]],\n",
        "\n",
        "       [[-0.39786366,  0.16201034, -0.19008337, -1.0602452 ,\n",
        "         -0.28693503,  0.09791763,  0.26614824,  0.41781986],\n",
        "        [ 0.35146567, -0.4469593 , -0.1841726 ,  0.45757473,\n",
        "         -0.61304873, -0.29104248, -0.16124889, -0.14987172],\n",
        "        [-0.2996967 ,  0.27353522, -0.09929767,  0.01973832,\n",
        "         -1.2312065 , -0.6374332 , -0.22963578,  0.55696607],\n",
        "        [-0.93818814,  0.02465284, -0.21117875,  1.1860403 ,\n",
        "         -0.06137538, -0.21062079, -0.094347  ,  0.50032747]]])\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-equGLTX7rxV"
      },
      "source": [
        "The sequence of patch vectors is processed by transformer encoder layers, each consisting of a self-attention and a feed-forward module. Since all vectors attend to one another, attention masking is not strictly necessary. However, we still implement it for the sake of consistency.\n",
        "\n",
        "Implement `TransformerEncoderLayer` in `cs231n/transformer_layers.py` and test it below. You should see relative error less than 1e-6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1372,
          "status": "ok",
          "timestamp": 1764596387894,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "-YIr_Fxv5xvy",
        "outputId": "ff5369ae-e716-4445-e0cf-adf69cb46e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error:  6.809801002433283e-07\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "from cs231n.transformer_layers import TransformerEncoderLayer\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "encoder_layer = TransformerEncoderLayer(D, 2, 4*D)\n",
        "x = torch.randn(N, T, D)\n",
        "x_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = encoder_layer(x, x_mask)\n",
        "\n",
        "expected_output = np.asarray([\n",
        "    [[-0.43529928, -0.204897, 0.45693663, -1.1355408, 1.8000772,\n",
        "      0.24467856, 0.8525885, -0.53586316, -1.5606489, -1.207276,\n",
        "      1.3986266, 0.3266182],\n",
        "     [0.06928468, 1.1030475, -0.9902548, -0.34333378, -2.1073136,\n",
        "      1.1960536, 0.16573538, -1.1772276, 1.2644588, -0.27311313,\n",
        "      0.29650143, 0.7961618],\n",
        "     [0.28310525, 0.69066685, -1.2264299, 1.0175265, -2.0517688,\n",
        "     -0.10330413, -0.5355796, -0.2696466, 0.13948536, 2.0408154,\n",
        "      0.27095756, -0.25582793],\n",
        "     [-0.58568114, 0.8019579, -0.9128079, -1.6816932, 1.1572194,\n",
        "      0.39162305, 0.58195484, 0.7043353, -1.27042, -1.1870497,\n",
        "      0.9784279, 1.0221335]]\n",
        "])\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1msDyvLseyI"
      },
      "source": [
        "Take a look at the `VisionTransformer` implementation in `cs231n/classifiers/transformer.py`.\n",
        "\n",
        "For classification, ViT divides the input image into patches and processes the sequence of patch vectors using a transformer. Finally, all the patch vectors are average-pooled and used to predict the image class. We will use the same 1D sinusoidal positional encoding to inject ordering information, though 2D sinusoidal and learned positional encodings are also valid choices.\n",
        "\n",
        "Complete the ViT forward pass and test it below. You should see relative error less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 490,
          "status": "ok",
          "timestamp": 1764597235109,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "AHZ1LMuRpOiV",
        "outputId": "2d8303d2-cce3-40ca-93b5-9028323c8136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scores error:  1.1475049635665582e-06\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "from cs231n.classifiers.transformer import VisionTransformer\n",
        "\n",
        "imgs = torch.randn(3, 3, 32, 32)\n",
        "transformer = VisionTransformer()\n",
        "scores = transformer(imgs)\n",
        "expected_scores = np.asarray(\n",
        "    [[-0.13013132,  0.13652277, -0.04656096, -0.16443546, -0.08946665,\n",
        "        -0.10123537,  0.11047452,  0.01317241,  0.17256221,  0.16230097],\n",
        "       [-0.11988413,  0.20006064, -0.04028708, -0.06937674, -0.07828291,\n",
        "        -0.13545093,  0.18698244,  0.01878054,  0.14309685,  0.03245382],\n",
        "       [-0.11540816,  0.21416159, -0.07740889, -0.08336161, -0.1645808 ,\n",
        "        -0.12318538,  0.18035144,  0.05492767,  0.15997584,  0.12134959]])\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Redma2A5BZA"
      },
      "source": [
        "\n",
        "We will first verify our implementation by overfitting it on one training batch. Tune learning rate and weight decay accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 24734,
          "status": "ok",
          "timestamp": 1764599460896,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "Myf9A3UK4TpM",
        "outputId": "a7b2a8df-f6ea-44f2-87a1-73c9dd5a5387"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170M/170M [00:08<00:00, 21.3MB/s] \n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10629,
          "status": "ok",
          "timestamp": 1764597374131,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "YU2pQigB9kxp",
        "outputId": "de9f4283-1417-47fa-e703-9f2a9698fb23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0/100] Loss 2.330371, Top-1 Accuracy: 0.078\n",
            "[10/100] Loss 2.108550, Top-1 Accuracy: 0.188\n",
            "[20/100] Loss 1.957598, Top-1 Accuracy: 0.281\n",
            "[30/100] Loss 1.807407, Top-1 Accuracy: 0.250\n",
            "[40/100] Loss 1.611219, Top-1 Accuracy: 0.422\n",
            "[50/100] Loss 1.210909, Top-1 Accuracy: 0.578\n",
            "[60/100] Loss 1.119390, Top-1 Accuracy: 0.562\n",
            "[70/100] Loss 0.705269, Top-1 Accuracy: 0.828\n",
            "[80/100] Loss 0.377919, Top-1 Accuracy: 0.969\n",
            "[90/100] Loss 0.146742, Top-1 Accuracy: 0.984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1.e-3\n",
        "\n",
        "\n",
        "batch = next(iter(DataLoader(train_data, batch_size=64, shuffle=False)))\n",
        "model = VisionTransformer(dropout=0.0)\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "model.train()\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    imgs, target = batch\n",
        "    out = model(imgs)\n",
        "    loss = loss_criterion(out, target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    top1 = (out.argmax(-1) == target).float().mean().item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"[{epoch}/{epochs}] Loss {loss.item():.6f}, Top-1 Accuracy: {top1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1764597377656,
          "user": {
            "displayName": "Yichen Lv",
            "userId": "13979448497646368557"
          },
          "user_tz": -480
        },
        "id": "8mrgWcfEE8XE",
        "outputId": "10e1cae0-145b-4fcf-d222-1d63cc21da35",
        "test": "vit_overfit_accuracy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overfitting ViT on one batch. Top-1 accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# You should get perfect 1.00 accuracy\n",
        "print(f\"Overfitting ViT on one batch. Top-1 accuracy: {top1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5-AVJNGYS9"
      },
      "source": [
        "Now we will train it on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8471b02ceae84a549fa99602271553ab",
            "0f11e7b4f46340e29988d0a7c5e14c63",
            "59e076e17dc944189494dac5d1785cff",
            "efff4c50022b45728cca39c854e5ec60",
            "139ec8e56b11416ba35ce7a68cce4c87",
            "c5ecde6422144f60929b5e535959297d",
            "5bc6c656c2ee4121b42b11c516bf84db",
            "0b33197b4b6f4be88e29e4e0f3ee4212",
            "e6740b1f4df14c298a0a0e0cbabb3094",
            "935f1d6d91404366b5623e1632871363",
            "0cd49de4a7c44c40922cfb126c4bbdf6",
            "b6f31eae85f04f178c650afddbde6124",
            "29a8840c4b3c4c44b2c854188e5c13d9",
            "7ddc504d838f402fa983baedb12f59cb",
            "ea78c71e44984df3a8999be76cba4e78",
            "a430b053ec1441d591c7a61561ba54dc",
            "9488398378be43faa70320cac9bc7fb8",
            "1d38dfc89ad84247abfe352f6aed09f1",
            "330807e01f9146dc953e46d42861a176",
            "61c729fcc3b54b04be3571256e7fd6e9",
            "6f2d45ebbe6849c0bbe8bc43d56ce7ec",
            "39d15d3ffb5043e3a663fe99f049e660",
            "2faf16a3343643b09b7b448056ee1e4e",
            "88dc1a1921dd42a6977bb63c3cb12625",
            "ec3d886340b946018bdf1a37a3cd0bed",
            "2e697c8abbca4aabb0808807bee15d8f",
            "b81e33f529c543318c1a1787762c5e47",
            "cc0b1edc45c5470391efdde2855300cd",
            "621da9a2f11446cc90ab92f741f478ce",
            "95f24dd112e0454280864c477cf216ad",
            "d1f529e8298a4bd88a65f8a62dd24bed",
            "98afe40ee2ec4bdcafd21c242c6201d5",
            "1da8f9eb028b4510b9e7509ab4cabe6c"
          ]
        },
        "id": "9W2dUKrz8MbG",
        "outputId": "541ce551-9434-44e0-8e8a-2bf79d8ae69d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch: [0/2] Loss: 1.8447 ACC@1: 0.321%: 100%|| 1563/1563 [00:29<00:00, 53.81it/s]\n",
            "Test Epoch: [0/2] Loss: 1.6705 ACC@1: 0.386%: 100%|| 313/313 [00:02<00:00, 140.29it/s]\n",
            "Train Epoch: [1/2] Loss: 1.5987 ACC@1: 0.423%: 100%|| 1563/1563 [00:28<00:00, 54.75it/s]\n",
            "Test Epoch: [1/2] Loss: 1.5226 ACC@1: 0.451%: 100%|| 313/313 [00:02<00:00, 133.67it/s]\n"
          ]
        }
      ],
      "source": [
        "from cs231n.classification_solver_vit import ClassificationSolverViT\n",
        "from cs231n.classifiers.transformer import VisionTransformer\n",
        "\n",
        "############################################################################\n",
        "# TODO: Train a Vision Transformer model that achieves over 0.45 test      #\n",
        "# accuracy on CIFAR-10 after 2 epochs by adjusting the model architecture  #\n",
        "# and/or training parameters as needed.                                    #\n",
        "#                                                                          #\n",
        "# Note: If you want to use a GPU runtime, go to `Runtime > Change runtime  #\n",
        "# type` and set `Hardware accelerator` to `GPU`. This will reset Colab,    #\n",
        "# so make sure to rerun the entire notebook from the beginning afterward.  #\n",
        "############################################################################\n",
        "\n",
        "learning_rate = 9e-4\n",
        "weight_decay = 1e-10\n",
        "batch_size = 32\n",
        "model = VisionTransformer(num_heads=16,num_layers=8)\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "\n",
        "solver = ClassificationSolverViT(\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    model=model,\n",
        "    num_epochs = 2,  # Don't change this\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay = weight_decay,\n",
        "    batch_size = batch_size,\n",
        ")\n",
        "\n",
        "solver.train('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "X1xe22cuGsnw",
        "test": "vit_test_accuracy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.4509\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy on test set: {solver.results['best_test_acc']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtjQqsDBm9or"
      },
      "source": [
        "# Inline Question 2\n",
        "\n",
        "Despite their recent success in large-scale image recognition tasks, ViTs often lag behind traditional CNNs when trained on smaller datasets. What underlying factor contribute to this performance gap? What techniques can be used to improve the performance of ViTs on small datasets?\n",
        "\n",
        "**Your Answer**:\n",
        "\n",
        "\n",
        "ViTs lack the inductive biases inherent in CNNs, such as translation equivariance and locality; it also flattens the image into patches, losing some spatial information.\n",
        "\n",
        "to improve performance on small datasets, we can pretrain on large datasets, apply regularization or even use a Vit mixed with CNN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cau-UmPBJ3ez"
      },
      "source": [
        "# Inline Question 3\n",
        "\n",
        "How does the computational cost of the self-attention layers in a ViT change if we independently make the following changes? Please ignore the computation cost of QKV and output projection.\n",
        "\n",
        "(i) Double the hidden dimension.\n",
        "\n",
        "(ii) Double the height and width of the input image.\n",
        "\n",
        "(iii) Double the patch size.\n",
        "\n",
        "(iv) Double the number of layers.\n",
        "\n",
        "\n",
        "**Your Answer**:\n",
        "\n",
        "Self-attention has $O(n^2d)$ complexity, where n is the sequence length $n = \\dfrac{H \\times W}{P^2}$ and d is the hidden dimension, so:\n",
        "\n",
        "1. 2x\n",
        "2. 16x\n",
        "3. 1/16x\n",
        "4. 2x"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b33197b4b6f4be88e29e4e0f3ee4212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd49de4a7c44c40922cfb126c4bbdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f11e7b4f46340e29988d0a7c5e14c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5ecde6422144f60929b5e535959297d",
            "placeholder": "",
            "style": "IPY_MODEL_5bc6c656c2ee4121b42b11c516bf84db",
            "value": "TrainEpoch:[0/2]Loss:1.8281ACC@1:0.329%:100%"
          }
        },
        "139ec8e56b11416ba35ce7a68cce4c87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d38dfc89ad84247abfe352f6aed09f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1da8f9eb028b4510b9e7509ab4cabe6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29a8840c4b3c4c44b2c854188e5c13d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9488398378be43faa70320cac9bc7fb8",
            "placeholder": "",
            "style": "IPY_MODEL_1d38dfc89ad84247abfe352f6aed09f1",
            "value": "TestEpoch:[0/2]Loss:1.6593ACC@1:0.405%:100%"
          }
        },
        "2e697c8abbca4aabb0808807bee15d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98afe40ee2ec4bdcafd21c242c6201d5",
            "placeholder": "",
            "style": "IPY_MODEL_1da8f9eb028b4510b9e7509ab4cabe6c",
            "value": "915/1563[00:30&lt;00:19,33.31it/s]"
          }
        },
        "2faf16a3343643b09b7b448056ee1e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88dc1a1921dd42a6977bb63c3cb12625",
              "IPY_MODEL_ec3d886340b946018bdf1a37a3cd0bed",
              "IPY_MODEL_2e697c8abbca4aabb0808807bee15d8f"
            ],
            "layout": "IPY_MODEL_b81e33f529c543318c1a1787762c5e47"
          }
        },
        "330807e01f9146dc953e46d42861a176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d15d3ffb5043e3a663fe99f049e660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59e076e17dc944189494dac5d1785cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b33197b4b6f4be88e29e4e0f3ee4212",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6740b1f4df14c298a0a0e0cbabb3094",
            "value": 1563
          }
        },
        "5bc6c656c2ee4121b42b11c516bf84db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61c729fcc3b54b04be3571256e7fd6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "621da9a2f11446cc90ab92f741f478ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f2d45ebbe6849c0bbe8bc43d56ce7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ddc504d838f402fa983baedb12f59cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330807e01f9146dc953e46d42861a176",
            "max": 313,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61c729fcc3b54b04be3571256e7fd6e9",
            "value": 313
          }
        },
        "8471b02ceae84a549fa99602271553ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f11e7b4f46340e29988d0a7c5e14c63",
              "IPY_MODEL_59e076e17dc944189494dac5d1785cff",
              "IPY_MODEL_efff4c50022b45728cca39c854e5ec60"
            ],
            "layout": "IPY_MODEL_139ec8e56b11416ba35ce7a68cce4c87"
          }
        },
        "88dc1a1921dd42a6977bb63c3cb12625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc0b1edc45c5470391efdde2855300cd",
            "placeholder": "",
            "style": "IPY_MODEL_621da9a2f11446cc90ab92f741f478ce",
            "value": "TrainEpoch:[1/2]Loss:1.6043ACC@1:0.419%:59%"
          }
        },
        "935f1d6d91404366b5623e1632871363": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9488398378be43faa70320cac9bc7fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f24dd112e0454280864c477cf216ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98afe40ee2ec4bdcafd21c242c6201d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a430b053ec1441d591c7a61561ba54dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f31eae85f04f178c650afddbde6124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29a8840c4b3c4c44b2c854188e5c13d9",
              "IPY_MODEL_7ddc504d838f402fa983baedb12f59cb",
              "IPY_MODEL_ea78c71e44984df3a8999be76cba4e78"
            ],
            "layout": "IPY_MODEL_a430b053ec1441d591c7a61561ba54dc"
          }
        },
        "b81e33f529c543318c1a1787762c5e47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ecde6422144f60929b5e535959297d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc0b1edc45c5470391efdde2855300cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f529e8298a4bd88a65f8a62dd24bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6740b1f4df14c298a0a0e0cbabb3094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea78c71e44984df3a8999be76cba4e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f2d45ebbe6849c0bbe8bc43d56ce7ec",
            "placeholder": "",
            "style": "IPY_MODEL_39d15d3ffb5043e3a663fe99f049e660",
            "value": "313/313[00:04&lt;00:00,56.46it/s]"
          }
        },
        "ec3d886340b946018bdf1a37a3cd0bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95f24dd112e0454280864c477cf216ad",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1f529e8298a4bd88a65f8a62dd24bed",
            "value": 915
          }
        },
        "efff4c50022b45728cca39c854e5ec60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_935f1d6d91404366b5623e1632871363",
            "placeholder": "",
            "style": "IPY_MODEL_0cd49de4a7c44c40922cfb126c4bbdf6",
            "value": "1563/1563[00:54&lt;00:00,34.45it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
